---
layout: post
title: "神经网络入门-感知器、反向传播等"
date: 2018-08-14
description: "神经网络基础知识点，感知器、反向传播、正则项、softmax、交叉熵"
tag: 深度学习
---



### 感知器--神经网络的起源

#### 神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造：

![images](/images/dl/47.png)



#### 感知器是激活函数为阶跃函数的神经元。感知器的模型如下：

![images](/images/dl/48.png)

是不是感觉很一样啊 ，神经元也叫做**感知器**。感知器算法在上个世纪50-70年代很流行，也成功解决了很多问题。并且，感知器算法也是非常简单的。 

### 感知器的定义

![images](/images/dl/48.png)

我们再来分析下上图这个感知器，可以看到，一个感知器有如下几个组成部分：

- **输入(inputs)**：一个感知器可以接收多个输入$(x_1,x_2,...,x_n \vert  x_i \in R)$

- **权值(weights)**：每一个输入上都有一个`权值`$w_i \in R$，此外还有一个**偏置项**$b \in R$，也就是上图的$w_0$。

- **加权和(weighted sum)**：就是`输入权值 x`    x    `权值 w`    +   `偏置项 b`的总和。

- **激活函数(step function)**：感知器的激活函数可以有很多选择。常见的有：

  - #### **`unit激活函数`**：$f(x)=unit(x)=  \begin{cases} 0&  x>0 \\ 1& x  \le 0 \end{cases} $

    ![images](/images/dl/49.png)

  - #### **`sigmod激活函数`**：$f(x)=sigmod(x)=\frac{1}{1+e^{-x}}$

    ![images](/images/dl/50.png)

  - ##### **`tanh激活函数`**：$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

    ​							    ![images](/images/dl/51.png)`

  - ##### `ReLU激活函数`：$f(x)=ReLU(x)=\begin{cases} x& x>0\\ 0& x \le 0 \end{cases} $

    ![images](/images/dl/52.png)

- **输出(output)**：感知器的输出由`加权值`用`激活函数`做`非线性变换`。也就是这个公式：$y=f(w\cdot x +b )$

#### 举个栗子：

我们使用`unit激活函数`结合上图就有：

- #### $y=f(w\cdot x +b )=f(w_1x_1+w_2x_2+w_3x_3+bias)$

- ##### 其中$f(x)$就是激活函数 $f(x)=  \begin{cases} 0&  x>0 \\ 1& x  \le 0 \end{cases}$ 。



### 感知器的前馈计算

再举个栗子：我们来计算下这个感知器：

![images](/images/dl/53.png)

其中`激活函数f`:$f(x)=  \begin{cases} 0&  x>0 \\ 1& x  \le 0 \end{cases}$

- `加权和`：logits  =  1.0 \* (-0.2) + 0.5 \* (-0.4) + (-1.4) \* 1.3 + 2.0 \* 3.0  =  1.98

- `输出值`：output = f(logits) = f(1.98) = 1

如果数据很多呢，我们就要把数据向量化：

例如：

#### $x_1=[-1.0, 3.0, 2.0] \\\ x_2=[2.0, -1.0, 5.0] \\\ x_3=[-2.0, 0.0, 3.0 ] \\\ x_4=[4.0, 1.0, 6.0] \\\ w=[4.0, -3.0, 5.0 ] \\\ b=2.0$

#### 则：$X=\begin{bmatrix}  -1.0 & 3.0 & 2.0 \\\ 2.0 & -1.0& 5.0 \\\ -2.0& 0.0& 3.0 \\\ 4.0& 1.0 & 6.0  \end{bmatrix}$

#### $w^T =\begin{bmatrix} 4.0 \\\ -3.0 \\\ 5.0 \end{bmatrix}$

#### 所以：$logits =  X\cdot w^T + b= \begin{bmatrix}  -1.0 & 3.0 & 2.0 \\\ 2.0 & -1.0& 5.0 \\\ -2.0& 0.0& 3.0 \\\ 4.0& 1.0 & 6.0  \end{bmatrix} \cdot \begin{bmatrix} 4.0 \\\ -3.0 \\\ 5.0 \end{bmatrix} + 2.0 \\\ =[-1.0 \ \ \  38.0 \ \ \ 7.0 \ \ \ 43.0 ]$

#### 最后带入激活函数：

#### 则：$output = f(x)=[0\ \ \ 1 \ \ \ 1 \ \ \ 1 ]$

