---
layout: post
title: "神经网络入门-感知器、反向传播等"
date: 2018-08-14
description: "神经网络基础知识点，感知器、反向传播、正则项、softmax、交叉熵"
tag: 深度学习
---



### 感知器--神经网络的起源

#### 神经网络是由人工神经元组成的。我们先来看看人的神经元是什么构造：

![images](/images/dl/47.png)



#### 感知器是激活函数为阶跃函数的神经元。感知器的模型如下：

![images](/images/dl/48.png)

是不是感觉很一样啊 ，神经元也叫做**感知器**。感知器算法在上个世纪50-70年代很流行，也成功解决了很多问题。并且，感知器算法也是非常简单的。 

### 感知器的定义

![images](/images/dl/48.png)

我们再来分析下上图这个感知器，可以看到，一个感知器有如下几个组成部分：

- **输入(inputs)**：一个感知器可以接收多个输入$(x_1,x_2,...,x_n \vert  x_i \in R)$

- **权值(weights)**：每一个输入上都有一个`权值`$w_i \in R$，此外还有一个**偏置项**$b \in R$，也就是上图的$w_0$。

- **加权和(weighted sum)**：就是`输入权值 x`    x    `权值 w`    +   `偏置项 b`的总和。

- **激活函数(step function)**：感知器的激活函数可以有很多选择。常见的有：

  - #### **`unit激活函数`**：$f(x)=unit(x)=  \begin{cases} 0&  x>0 \\ 1& x  \le 0 \end{cases} $

    ![images](/images/dl/49.png)

  - #### **`sigmod激活函数`**：$f(x)=sigmod(x)=\frac{1}{1+e^{-x}}$

    ![images](/images/dl/50.png)

  - ##### **`tanh激活函数`**：$f(x)=tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}$

    ​							    ![images](/images/dl/51.png)`

  - ##### ReLU激活函数`：$f(x)=ReLU(x)=\begin{cases} x& x>0\\ 0& x \le 0 \end{cases} $

    ![images](/images/dl/52.png)

- **输出(output)**：感知器的输出由`加权值`用`激活函数`做`非线性变换`。也就是这个公式：$y=f(w\cdot x +b )$

#### 举个栗子：

我们使用`unit激活函数`结合上图就有：

- #### $y=f(w\cdot x +b )=f(w_1x_1+w_2x_2+w_3x_3+bias)$

- ##### 其中$f(x)$就是激活函数 $f(x)=  \begin{cases} 0&  x>0 \\ 1& x  \le 0 \end{cases}$ 。



### 感知器的前馈计算

再举个栗子：我们来计算下这个感知器：

![images](/images/dl/53.png)