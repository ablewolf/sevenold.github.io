---
layout: post
title: "深度学习之卷积神经网络"
date: 2018-08-21
description: "卷积神经网络基础知识点，卷积运算、池化、卷积神经网络"
tag: 深度学习
---



### 卷积神经网络（Convolutional Neural Network, CNN） 简介

`卷积神经网络`是近年来广泛应用于模式识别、图像处理等领域的一种高效识别算法，它具有结构简单、训练参数少和适应性强等特点。

`卷积神经网络`是一种前馈神经网络，它的人工神经元可以响应一部分覆盖范围内的周围单元，对于大型图像处理有出色表现。

`卷积神经网络`由一个或多个卷积计算层和顶端的全连接层（经典神经网络）组成，同时也包括关联权重和池化层。这一结构是的卷积神经网络能够利用输入数据的二维结构。与其他神经网络结构相比，卷积神经网络在图像和语音识别方面能够给出更优的结果。相比较其他深度、前馈神经网络，卷积神经网络需要估计的参数更少，从而使之成为一种颇具吸引力的深度学习框架。

我们前面所接触到的神经网络的结构是这样的：

![images](/images/dl/65.png)

卷积神经网络依旧是层级网络，只是层的功能和形式做了变化，也可以是传统神经网络的一个改进，不同的层次有不同运算与功能，如下图中就多了很多传统神经网络中所没有的层次。

![images](/images/dl/66.png)



### 卷积神经网络的核心思想

- `局部感受野`：普通的多层感知器中，隐层结节点会全连接到一个图像的每个像素点上；而在卷积神经网络中，每个隐层的节点只连接到图像某个足够小的像素点上，从而大大减少需要训练的权重参数。
- `权值共享`：在卷积神经网中，同一个卷积核内，所有的神经元的权值是相同的，从而大大减少需要训练的参数。
-  `池化`：在卷积神经网络中，没有必要一定就要对原图像做处理，而是可以使用某种“压缩”方法，这就是池化，也就是每次将原图像卷积后，都通过一个下采样的过程，来减小图像的规模。 



### 卷积神经网络的层级结构

![images](/images/dl/71.png)

- #### **数据输入层（Input layer）**

- #### **卷积计算层（CONV layer）**

- #### **ReLU激活层（ReLU layer）**

- #### **池化层（POOling layer）**

- #### **全连接层（FC layer）**

  

### **数据输入层（Input layer）**

#### `任务`：主要是对原始图像进行预处理

- #### `去均值`：把输入数据的各个维度都中心化到0。

- #### `归一化`：把数据的幅度归一化到同样的范围。

- #### `PCA/白化`：用PCA降维、白化是对数据的每个特征轴上的幅度归一化。

#### 举个栗子：

#### `去均值与归一化`

去均值的目的在于把样本的中心拉回到坐标系原点上；归一化的目的就是减少各维度数据取值范围的差异而带来的干扰。比如，我们有两个维度的特征A和B，A范围是0到10，而B范围是0到10000，如果直接使用这两个特征是有问题的，好的做法就是归一化，即A和B的数据都变为0到1的范围。 

![images](/images/dl/67.png)

#### `去相关与白化`

![images](/images/dl/68.png)

### **卷积计算层（CONV layer）**

#### `任务`：对输入的图像进行特征提取

卷积计算层是卷积神经网络中最重要的一个层次，也是“卷积神经网络”的名字来源。

#### 举个栗子：

假设一张图像有 5x5 个像素，1 代表白，0 代表黑，这幅图像被视为 5x5 的单色图像。现在用一个由随机地 0 和 1 组成的 3x3 矩阵去和图像中的子区域做Hadamard乘积，每次迭代移动一个像素，这样该乘法会得到一个新的 3x3 的矩阵。下面的动图展示了这个过程。 

![images](/images/dl/69.gif)

#### **直观上来理解**：

- 用一个小的权重矩阵去覆盖输入数据，对应位置元素加权相乘，其和作为结果的一个像素点。
- 这个权重在输入数据上滑动，形成一张新的矩阵
- 这个权重矩阵就被称为`卷积核`（convolution kernel）
- 其覆盖的位置称为`感受野`（receptive fileld ）
- 生成的新矩阵叫做`特征图`（feature map）

分解开来，就如下图所示：

![images](/images/dl/72.png)

#### 其中：

- 滑动的像素数量就叫做`步长`（stride）
- 以卷积核的边还是中心点作为开始/结束的依据，决定了卷积的`补齐`（padding）方式。前面我们所举的栗子是`valid`方式，而`same`方式则会在图像的边缘用0补齐，如将前面的`valid`改为`same`方式，如图所示：

![images](/images/dl/73.png)

- 我们前面所提到的输入图像都是灰色的，只有一个通道，但是我们一般会遇到输入通道不只有一个，那么卷积核是三阶的，也就是说所有的通道的结果做累加。

![images](/images/dl/74.png)

#### 再举个栗子：

![images](/images/dl/70.gif)



### **ReLU激活层（ReLU layer）**

#### `任务`：卷积后的结果压缩到某一个固定的范围，这样可以一直保持一层一层下去的数值范围是可控的。 



### **池化层（POOling layer）**

#### `任务`：对特征进行采样，即用一个数值替代一块区域，主要是为了降低网络训练参数及模型的过拟合程度。 



### 标准化层（Standardized layer ）

#### `任务`：数据归一化。



### **全连接层（FC layer）**

#### `任务`：全连接层的每一个结点都与上一层的所有结点相连，用来把前边提取到的特征综合起来。由于其全相连的特性，一般全连接层的参数也是最多的。主要是为了分类或回归，当然也可以没有。 



### Dropout层（Dropout layer）

#### `任务`：在模型训练时随机让网络某些隐含层节点的权重不工作，不工作的那些节点可以暂时认为不是网络结构的一部分，但是它的权重得保留下来（只是暂时不更新而已），因为下次样本输入时它可能又得工作了。主要是为了防止过拟合。

### 

