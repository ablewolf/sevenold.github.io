---
layout: post
title: "神经网络之对抗生成网络"
date: 2018-09-1
description: "对抗生成网络、GAN"
tag: 深度学习
---

### 对抗生成网络

生成式对抗网络（GAN, Generative Adversarial Networks ）是一种**深度学习**模型，是近年来复杂分布上无监督学习最具前景的方法之一。

模型通过框架中（至少）两个模块：**生成模型（Generative Model）和判别模型（Discriminative Model）**的互相**博弈**学习产生相当好的输出。

原始 GAN 理论中，并不要求 G 和 D 都是神经网络，只需要是能拟合相应生成和判别的函数即可。

但实用中一般均使用深度神经网络作为 G 和 D 。一个优秀的GAN应用需要有良好的训练方法，否则可能由于神经网络模型的自由性而导致输出不理想。



### GAN的基本框架

GAN所建立的一个学习框架，实际上我们可以看成**生成模型和判别模型**之间的一个模拟对抗游戏。我们可以把**生成模型**看作一个**伪装者**，而把**判别模型**看成一个**警察**。**生成模型**通过不断地学习来提高自己的**伪装能力**，从而使得生成出来的数据能够更好地“欺骗”**判别模型**。而**判别模型**则通过不断的训练来提高自己的判别能力，能够更准确地判断出数据的来源。GAN就是这样一个不断对抗的网络。GAN的架构如下图所示：

![images](/images/dl/110.png)

**生成模型**以**随机变量**作为输入，其输出是对真实数据分布的一个估计。

生成数据和真实数据的采样都由**判别模型**进行判别，并给出真假性的判断和当前的损失。

利用**反向传播**，GAN对生成模型和判别模型进行交替优化。

![images](/images/dl/113.png)



### GAN的优化目标

在对抗生成网络中，有两个博弈的角色分别为**生成式模型(generative model)和判别式模型(discriminative model)**。具体方式为：

- 生成模型G捕捉样本数据的分布，判别模型D时一个二分类器，估计一个样本来自于训练数据（而非生成数据）的概率。

在博弈的过程中我们需要提高两个模型的能力，所以通过不断调整**生成模型G**和**判别模型D**，直到**判别模型D**不能把数据的真假判别出来为止。在调整优化的过程中，我们需要：

- 优化**生成模型G**，使得**判别模型D**无法判别出来事件的真假。
- 优化**判别模型D**，使得它尽可能的判别出事件的真假。

### 举个栗子：

假设数据的概率分布为M，但是我们不知道具体的分布和构造是什么样的，就好像是一个黑盒子。为了了解这个黑盒子，我们就可以构建一个对抗生成网络：

- **生成模型G**：使用一种我们完全知道的概率分布N来不断学习成为我们不知道的概率分布M.
- **判别模型D**：用来判别这个不断学习的概率是我们知道的概率分布N还是我们不知道的概率分布M。

我们用图像来体现：

![images](/images/dl/111.png)

![images](/images/dl/112.png)



由上图所示：

- 黑点所组成的数据分布是我们所不知道的概率分布M所形成的
- 绿色的线表示**生成模型G**使用已知的数据和判别模型不断对抗生成的数据分布。
- 蓝色的线表示**判断模型D**
- a图：初始状态
- b图：生成模型不变，优化判别模型，直到判别的准确率最高
- c图：判别模型不变。优化生成模型。直到生成的数据的真实性越高
- d图：多次迭代后，生成模型产生的数据和概率部分M的数据基本一致，从而判别模型认为生成模型生成的数据就是概率分布M的数据分布。



### GAN的数学推导

#### **GAN的目标函数**：

![images](/images/dl/114.png)

GAN的训练是目标函数的最大化判别模型和最小化生成模型的对抗游戏：

- #### $P_{data}$是真实的数据。

- #### $P_z$是生成数据

当我们训练判别器D时，我们希望真实数据的判别值越大越好。

同时我们希望对生成数据的判别值越小越好，所以也是越大越好。

训练中使用梯度上升，使目标函数的值越来越高。

同理，当我们要训练生成器G时，就希望价值函数的值越小越好，即使用梯度下降来训练生成器的参数。

 两个模型相对抗，最后达到全局最优。

#### **训练算法**：

首先固定G，单独训练D，为了让D得到充分训练，有的时候要迭代多次。

每一轮迭代D只训练一次。

D训练完毕后，固定D，训练G，如此循环。

训练的方式是反向传播算法。

![images](/images/dl/115.png)



#### 当$P_g=P_{data}$时，为全局最优：

1. 在固定生成模型G时，判别模型D的最优解：

![images](/images/dl/116.png)

从而，目标函数就可以化简为：

#### $a \cdot log(y)+ b \cdot log(1-y)$

该目标函数的最优解：

#### $\frac{a}{a+b}$



2. 最小化的部分就可以重新定义为：

   ![images](/images/dl/117.png)



从而我们就可以证明，当$P_g=P_{data}$时，为全局最优，此时判别器D无论对真实样本还是虚假样本，判别的结果都是0.5的概率。也就达到了我们所需要的效果。



### GAN的特性

优点：

- 模型优化只用到了反向传播，而不需要马尔科夫链。
- 训练时不需要对隐变量做推断。
- 理论上，只要是可微分函数都能用于构建生成模型G和判别模型D，因而能够与深度神经网络结合-->深度产生式模型。
- 生成模型G的参数更新不是直接来自于数据样本，而是使用来自判别模型D的反向传播梯度。

缺点：

- 可解释性差，生成模型的分布没有显示的表达。它只是一个黑盒子一样的映射函数：输入是一个随机变量，输出是我们想要的一个数据分布。
- 比较难训练，生成模型D和判别模型G之间需要很好的同步。例如，在实际中我们常常需要 D 更新 K次， G 才能更新 1 次，如果没有很好地平衡这两个部件的优化，那么G最后就极大可能会坍缩到一个鞍点。

