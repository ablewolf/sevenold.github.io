---
layout: post
title: "机器学习-决策树和随机森林"
date: 2018-07-23
description: "决策树和随机森林的推导总结"
tag: 机器学习
---   

### 决策树

**决策树（decision tree）**是一种**分类**与**回归**方法，本文主要讨论用于**分类**的决策树，决策树的结构呈**树形**结构，在分类问题中，其代表基于特征对数据进行分类的过程，通常可以认为是if-then规则的集合，也可以认为是定义在**特征空间与类空间上的条件概率分布**。其主要优点是模型可读性好并且分类速度快。训练的时候，利用训练数据根据损失函数最小化的原则建立决策树模型。

预测时对于新的数据，利用决策树进行分类。决策树的学习通常包括三个步骤：**特征选择，生成决策树，对决策树进行剪枝**。这些决策树的思想主要来自Quinlan在1986年提出的ID3算法和1993年提出的C4.5算法，以及Breiman等人在1984年提出的CART算法。 

用于分类的决策树是一种对数据进行分类的树形结构。决策树主要由节点（node）和有向边（directed edge）组成。节点有两种类型：内部节点（internal node）以及叶节点（leaf node）。内部节点表示一个特征或者属性，叶节点表示一个类。其结构如图所示：  

![image](/images/ml/14.png)

决策树学习采用的是**自顶向下**的递归方法, 其基本思想是以**信息熵**为度量构造一棵**熵值 下降最快**的树,到叶子节点处的熵值为零, 此时每个叶节点中的实例都属于同一类。

- 最大优点: 可以自学习。在学习的过程中,不需要使用者了解过多背景知识,只需要对训练实例进行较好的标注,就能够进行学习。
- 显然,属于**有监督学习**。



### 决策树三种生成算法

1. ID3 --- **信息增益** **最大**的准则

2. C4.5 --- **信息增益比** 最大的准则

3. CART

   - 回归树: **平方误差** **最小** 的准则

   - 分类树: **基尼系数** **最小**的准则



### 信息熵

#### **信息熵（information entropy）** 是度量样本集合纯度的最常用的一种指标。

- #### 熵：$H(X)=-\sum_{x \in X} p(x,y)logp(x)$

- #### 联合熵：$H(X，Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x,y)$

- #### 条件熵：$H(X|Y)=-\sum_{x \in X,y \in Y} p(x,y)logp(x|y)$

- #### 相对熵：$D(p||q)=\sum_x p(x)log\frac{p(x)}{q(x)}$

- #### 互信息：$I(x,y)=\sum_{x\in X, y \in Y} p(x,y )log\frac{p(x,y)}{p(x)p(y)}$



### 决策树学习基本算法

![image](/images/ml/15.png)

### 信息增益---$g(D,A)$

定义: 特征A对训练数据集D的信息增益 g(D,A), 定义为集合D的经验熵H(D)与特征A给定条件下D的经验条件熵$H(D|A)$之差,即: $$g(D,A)=H(D)–H(D|A) $$






转载请注明：[Seven的博客](http://seven.github.io) » [点击阅读原文](https://sevenold.github.io/2018/07/math/)
